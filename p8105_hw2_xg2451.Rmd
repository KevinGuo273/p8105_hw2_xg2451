---
title: "p8105_hw2_xg2451"
author: "Xuanyu Guo"
date: "2024-09-27"
output: html_document
---
# Problem1
```{r message=FALSE}
library(dplyr)
library(tidyr)

# Load data
data <- read.csv("NYC_Transit_Subway_Entrance_And_Exit_Data.csv")

# Select and clean the required columns
clean_data <- data %>%
  select(Line, Station.Name, Station.Latitude, Station.Longitude,
         Route1:Route11, Entry, Vending, Entrance.Type, ADA) %>%
  mutate(Entry = ifelse(Entry == "YES", TRUE, FALSE))
```
```{r}
# Check dimensions and print a summary
nrow(clean_data)
ncol(clean_data)
```
The dataset contains variables related to NYC subway stations, including line, station name, station latitude and longitude, up to 11 routes served by each station, whether the entrance allows entry, whether there is a vending machine, the type of entrance, and ADA compliance. The data cleaning involved retaining relevant columns and converting the Entry variable from a character string to a logical variable. The resulting dataset has dimensions indicating the number of 1868 rows and 19 columns. The data structure has been arranged to meet the principles of tidy data.

```{r}
# Calculate the number of distinct stations
distinct_stations <- clean_data %>%
  distinct(Line, Station.Name) %>%
  nrow()
distinct_stations
```
There are 465 distinct stations.
```{r}
# Calculate how many stations are ADA compliant
ada_compliant_stations <- clean_data %>%
  filter(ADA == TRUE) %>%
  distinct(Line, Station.Name) %>%
  nrow()
ada_compliant_stations
```
There are 84 ADA compliant stations.
```{r}
# Proportion of station entrances/exits without vending that allow entrance
proportion_no_vending_entry <- clean_data %>%
  filter(Vending == "NO") %>%
  summarise(Proportion = mean(Entry))
proportion_no_vending_entry
```
The proportion of station entrances / exits without vending allow entrance is approximately 37.70%.
```{r}
# Convert all Route types to character and all "" to NA
clean_data <- clean_data %>%
  mutate(across(starts_with("Route"), ~na_if(as.character(.), "")))
# Reformat data
reformat_data <- clean_data %>%
  pivot_longer(cols = Route1:Route11, values_to = "Route", names_to = "Route_Num", values_drop_na = TRUE)
```
```{r}
# Separate route numbers into distinct variables and calculate A train stations
a_train_stations <- reformat_data %>%
  filter(Route == "A") %>%
  distinct(Line, Station.Name) %>%
  nrow()
a_train_stations
# ADA compliant stations serving the A train
a_train_ada_compliant <- reformat_data %>%
  filter(Route == "A", ADA == TRUE) %>%
  distinct(Line, Station.Name) %>%
  nrow()
a_train_ada_compliant
```
There are 60 distinct stations serve the A train, and 17 of them are ADA compliant.

# Problem2
```{r message=FALSE}
library(readxl)
# Read the data for Mr. Trash Wheel
mr_trash_wheel <- read_excel("Trash_Wheel_Collection_Data.xlsx", sheet = "Mr. Trash Wheel") %>%
  select(-c(...15, ...16)) %>%  # Assuming columns 15 and 16 are notes
  filter(!is.na(Dumpster)) %>%  # Remove rows without dumpster data
  rename( # omit columns containing notes
    Weight = "Weight (tons)",
    Volume = "Volume (cubic yards)",
    "Homes Powered" = "Homes Powered*"
    ) %>%
  # round and convert the result to an integer variable
  mutate(Year = as.numeric(Year),
         Sports_Balls = as.integer(round(`Sports Balls`)),
         Trash_Wheel = "Mr. Trash Wheel")


# Read the data for Professor Trash Wheel
prof_trash_wheel <- read_excel("Trash_Wheel_Collection_Data.xlsx", sheet = "Professor Trash Wheel") %>%
  filter(!is.na(Dumpster)) %>%  
  rename( 
    Weight = "Weight (tons)",
    Volume = "Volume (cubic yards)",
    "Homes Powered" = "Homes Powered*"
    ) %>%
  mutate(Trash_Wheel = "Professor Trash Wheel")

# Read the data for Gwynnda Trash Wheel
gwynnda_trash_wheel <- read_excel("Trash_Wheel_Collection_Data.xlsx", sheet = "Gwynnda Trash Wheel") %>%
  filter(!is.na(Dumpster)) %>%  
  rename(
    Weight = "Weight (tons)",
    Volume = "Volume (cubic yards)",
    "Homes Powered" = "Homes Powered*"
    ) %>%
  mutate(Trash_Wheel = "Gwynnda Trash Wheel")
```
```{r}
combined_data <- bind_rows(mr_trash_wheel, prof_trash_wheel, gwynnda_trash_wheel)
                           
# Total observations in the dataset
total_observations <- nrow(combined_data)
total_observations
# Total weight of trash collected by Professor Trash Wheel
total_weight_prof <- sum(prof_trash_wheel$Weight, na.rm = TRUE)
total_weight_prof
# Total number of cigarette butts collected by Gwynnda in June 2022
cigarette_butts_june2022 <- sum(gwynnda_trash_wheel$`Cigarette Butts`[gwynnda_trash_wheel$Month == "June" & gwynnda_trash_wheel$Year == 2022], na.rm = TRUE)
cigarette_butts_june2022
```
The combined dataset for Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda consists of `r total_observations` observations, capturing various metrics of waste collected from Baltimore's waterways. For instance, key variables include the weight of trash in tons and the number of specific waste items like cigarette butts and sports balls. In total, Professor Trash Wheel collected `r total_weight_prof` tons of trash. In June 2022, Gwynnda collected `r format(cigarette_butts_june2022)` cigarette butts, indicating the substantial impact these devices have on reducing aquatic litter.

# Problem3

```{r results='hide'}
library(readr)
dat_bakers = read_csv("./gbb_datasets/bakers.csv") %>% 
  janitor::clean_names() %>% 
  separate(
    baker_name, 
    into = c("baker", "baker_last_name"), 
    sep = " "
  ) 

dat_bakes = read_csv("./gbb_datasets/bakes.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    baker = ifelse(baker == "\"Jo\"", "Jo", baker)
  )

dat_results = read_csv("./gbb_datasets/results.csv", skip = 2) %>% 
  janitor::clean_names() %>% 
  filter(!is.na(result)) %>% 
  mutate(
    baker = ifelse(baker == "Joanne", "Jo", baker)
  )
# check 
anti_join(dat_bakers, dat_results)
anti_join(dat_bakers, dat_bakes)
# merge
dat_merge1 = left_join(dat_results, dat_bakes) 
dat_merge2 = left_join(dat_merge1, dat_bakers)
# organize in meaningful orders
dat_merge = dat_merge2 %>% 
  distinct() %>% 
  select(series, episode, baker, signature_bake, technical, result, 
         show_stopper, everything())
```
```{r}
# summary
skimr::skim(dat_merge)
# write csv
write_csv(dat_merge, "./gbb_datasets/merge_data.csv")
```

**Cleaning progress:**

- In dataset dat_bakers, the variable "baker_name" had different name and form 
from other two datasets, so separate this variable into two columns "baker" and "baker_last_name".
- In dataset dat_bakes, "Jo" in variable "baker" had a quotation marks, so remove it. 
- In dataset dat_results, omit the first two useless rows and change "Joanne"
in column "baker" to "Jo". 

**Briefly Discussion:**

There are 710 rows and 11 columns, including 7 character variables (eg. baker, signature_bake, result) and 4 numeric variables (eg. series, episode, technical). 
Variables "signature_bake", "show_stopper" and "technical" have missing values.

**Star bakers / Winners:**

```{r}
star_baker = dat_merge %>% 
  filter(series >= 5 & series <= 10 & result %in% c("STAR BAKER", "WINNER")) %>% 
  select(series, episode, result, baker) %>% 
  arrange(series, episode)
star_baker
```
  Overall Winners in every series:

- Richard obtained 5 star baker in series 5.
- Nadiya obtained 4 star baker in series 6. 
- Candice obtained 4 star baker in series 7. 
- Steven and Sophie both obtained 3 star baker in series 8. 
- Rahul obtained 3 star baker in series 9. 
- Steph obtained 4 star baker in series 10. 
- **It is not surprising that they are winners.**

**Viewers:**

```{r}
dat_viewers = read_csv("./gbb_datasets/viewers.csv") %>% 
  janitor::clean_names()

head(dat_viewers, 10) #the first 10 rows
```

```{r}
# average viewership in Season 1
average_1 = mean(dat_viewers$series_1, na.rm = TRUE) 
average_1
# Season 5
average_5 = mean(dat_viewers$series_5, na.rm = TRUE)
average_5
```

The average viewership in Season 1 is 2.77. 
\
The average viewership in Season 5 is 10.04. 

















